{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Step 0 - Preprocessing</h1></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we first read the data including the bug reports and source code files of all 51 projects and for ease of access, we save them as two pickle files in the ./Data directory. Therefore, this set of code will populate the ./Data directory with \"allBugReports.pickle\" which is a pandas dataframe that contains all the bug reports from all projects and \"allSourceCodes.pickle\" which is a pandas dataframe that contains all source files after preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "# import ray.data as ray_pd\n",
    "import modin.pandas as ray_pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import warnings\n",
    "import javalang\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "from scipy import spatial\n",
    "import scipy.spatial.distance\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import multiprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from time import gmtime, strftime\n",
    "from random import randint\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from pathlib import Path\n",
    "import zlib\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-25 15:10:52,474\tERROR services.py:1254 -- Failed to start the dashboard: Failed to read dashbord log: [Errno 2] No such file or directory: '/tmp/ray/session_2022-02-25_15-10-31_279754_18960/logs/dashboard.log'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.138.255',\n",
       " 'raylet_ip_address': '172.16.138.255',\n",
       " 'redis_address': '172.16.138.255:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2022-02-25_15-10-31_279754_18960/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-02-25_15-10-31_279754_18960/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-02-25_15-10-31_279754_18960',\n",
       " 'metrics_export_port': 60327,\n",
       " 'node_id': 'c834043192deff829aca70f183500955aab1e8befbf9d48fd5668232'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Splitting code and natural language</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_natural_lang(doc):\n",
    "    \"\"\"\n",
    "    @Receives: a document in natural language (bugreport)\n",
    "    @Process: splits it as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wordList=[]\n",
    "        word=''\n",
    "        for char in doc:\n",
    "            if char.isalnum() or char=='\\'':\n",
    "                word+=char\n",
    "            else:\n",
    "                if len(word)>0:\n",
    "                    wordList.append(word)\n",
    "                    word=''\n",
    "        if len(word)>0:\n",
    "            wordList.append(word)\n",
    "        return wordList\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def code_splitter(sourceCode):\n",
    "    \"\"\"\n",
    "    @Receives: a code\n",
    "    @Process: splits it same as described in BugLocator\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    contentBuf = []\n",
    "    wordBuf = []\n",
    "    for char in sourceCode:\n",
    "        if ((char >= 'a' and char <= 'z') or (char >= 'A' and char <= 'Z')):\n",
    "            wordBuf.append(char)\n",
    "            continue\n",
    "        length = len(wordBuf)\n",
    "        if (length != 0):\n",
    "            k = 0\n",
    "            for i in range(length-1):\n",
    "                j=i+1\n",
    "                first_char = wordBuf[i]\n",
    "                second_char = wordBuf[j]\n",
    "                if ((first_char >= 'A' and first_char <= 'Z') and (second_char >= 'a' and second_char <= 'z')):\n",
    "                    contentBuf.append(wordBuf[k:i])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = i\n",
    "                    continue\n",
    "                if ((first_char >= 'a' and first_char <= 'z') and (second_char >= 'A' and second_char <= 'Z')):\n",
    "                    contentBuf.append(wordBuf[k:j])\n",
    "                    contentBuf.append(' ')\n",
    "                    k = j\n",
    "                    continue\n",
    "            if (k < length):\n",
    "                contentBuf.append(wordBuf[k:])\n",
    "                contentBuf.append(\" \")\n",
    "            wordBuf=[]\n",
    "    words=''\n",
    "    for each in contentBuf:\n",
    "        if isinstance(each,str):\n",
    "            words+=each\n",
    "        else: \n",
    "            for term in each: \n",
    "                words+=term\n",
    "    words= words.split()\n",
    "    contentBuf = []\n",
    "    for i in range(len(words)):\n",
    "        if (words[i].strip()!=\"\" and len(words[i]) >= 2):\n",
    "            contentBuf.append(words[i])\n",
    "    return contentBuf\n",
    "\n",
    "\n",
    "\n",
    "def general_preprocessor(doc,mode):\n",
    "    \"\"\"\n",
    "    @Receives: a document (code or bug report denoted by mode)\n",
    "    @Process: processes the docucument by stemming and removing stop-words and converting to lower cases\n",
    "    @Return: a list of lower cased words\n",
    "    \"\"\"\n",
    "    JavaKeywords=[\"abstract\", \"continue\", \"for\", \n",
    "                \"new\", \"switch\", \"assert\", \"default\", \"goto\", \"package\", \n",
    "                \"synchronized\", \"boolean\", \"do\", \"if\", \"private\", \"this\", \n",
    "                \"break\", \"double\", \"implements\", \"protected\", \"throw\", \"byte\", \n",
    "                \"else\", \"import\", \"public\", \"throws\", \"case\", \"enum\", \n",
    "                \"instanceof\", \"return\", \"transient\", \"catch\", \"extends\", \"int\", \n",
    "                \"short\", \"try\", \"char\", \"final\", \"interface\", \"static\", \"void\", \n",
    "                \"class\", \"finally\", \"long\", \"strictfp\", \"volatile\", \"const\", \n",
    "                \"float\", \"native\", \"super\", \"while\", \"org\", \"eclipse\", \"swt\", \n",
    "                \"string\", \"main\", \"args\", \"null\", \"this\", \"extends\", \"true\", \n",
    "                \"false\"]\n",
    "    stop_words=[\"a\", \"a's\", \"able\", \"about\", \"above\",\n",
    "                \"according\", \"accordingly\", \"across\", \"actually\", \"after\",\n",
    "                \"afterwards\", \"again\", \"against\", \"ain't\", \"all\", \"allow\",\n",
    "                \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\n",
    "                \"although\", \"always\", \"am\", \"among\", \"amongst\", \"an\", \"and\",\n",
    "                \"another\", \"any\", \"anybody\", \"anyhow\", \"anyone\", \"anything\",\n",
    "                \"anyway\", \"anyways\", \"anywhere\", \"apart\", \"appear\",\n",
    "                \"appreciate\", \"appropriate\", \"are\", \"aren't\", \"around\", \"as\",\n",
    "                \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"available\",\n",
    "                \"away\", \"awfully\", \"b\", \"be\", \"became\", \"because\", \"become\",\n",
    "                \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\",\n",
    "                \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\",\n",
    "                \"best\", \"better\", \"between\", \"beyond\", \"both\", \"brief\", \"but\",\n",
    "                \"by\", \"c\", \"c'mon\", \"c's\", \"came\", \"can\", \"can't\", \"cannot\",\n",
    "                \"cant\", \"cause\", \"causes\", \"certain\", \"certainly\", \"changes\",\n",
    "                \"clearly\", \"co\", \"com\", \"come\", \"comes\", \"concerning\",\n",
    "                \"consequently\", \"consider\", \"considering\", \"contain\",\n",
    "                \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn't\",\n",
    "                \"course\", \"currently\", \"d\", \"definitely\", \"described\",\n",
    "                \"despite\", \"did\", \"didn't\", \"different\", \"do\", \"does\",\n",
    "                \"doesn't\", \"doing\", \"don't\", \"done\", \"down\", \"downwards\",\n",
    "                \"during\", \"e\", \"each\", \"edu\", \"eg\", \"eight\", \"either\", \"else\",\n",
    "                \"elsewhere\", \"enough\", \"entirely\", \"especially\", \"et\", \"etc\",\n",
    "                \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\",\n",
    "                \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"f\", \"far\",\n",
    "                \"few\", \"fifth\", \"first\", \"five\", \"followed\", \"following\",\n",
    "                \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"four\",\n",
    "                \"from\", \"further\", \"furthermore\", \"g\", \"get\", \"gets\",\n",
    "                \"getting\", \"given\", \"gives\", \"go\", \"goes\", \"going\", \"gone\",\n",
    "                \"got\", \"gotten\", \"greetings\", \"h\", \"had\", \"hadn't\", \"happens\",\n",
    "                \"hardly\", \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\",\n",
    "                \"he's\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"here's\",\n",
    "                \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\",\n",
    "                \"hi\", \"him\", \"himself\", \"his\", \"hither\", \"hopefully\", \"how\",\n",
    "                \"howbeit\", \"however\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"ie\",\n",
    "                \"if\", \"ignored\", \"immediate\", \"in\", \"inasmuch\", \"inc\",\n",
    "                \"indeed\", \"indicate\", \"indicated\", \"indicates\", \"inner\",\n",
    "                \"insofar\", \"instead\", \"into\", \"inward\", \"is\", \"isn't\", \"it\",\n",
    "                \"it'd\", \"it'll\", \"it's\", \"its\", \"itself\", \"j\", \"just\", \"k\",\n",
    "                \"keep\", \"keeps\", \"kept\", \"know\", \"knows\", \"known\", \"l\", \"last\",\n",
    "                \"lately\", \"later\", \"latter\", \"latterly\", \"least\", \"less\",\n",
    "                \"lest\", \"let\", \"let's\", \"like\", \"liked\", \"likely\", \"little\",\n",
    "                \"look\", \"looking\", \"looks\", \"ltd\", \"m\", \"mainly\", \"many\", \n",
    "                \"may\", \"maybe\", \"me\", \"mean\", \"meanwhile\", \"merely\", \"might\",\n",
    "                \"more\", \"moreover\", \"most\", \"mostly\", \"much\", \"must\", \"my\",\n",
    "                \"myself\", \"n\", \"name\", \"namely\", \"nd\", \"near\", \"nearly\",\n",
    "                \"necessary\", \"need\", \"needs\", \"neither\", \"never\",\n",
    "                \"nevertheless\", \"new\", \"next\", \"nine\", \"no\", \"nobody\", \"non\",\n",
    "                \"none\", \"noone\", \"nor\", \"normally\", \"not\", \"nothing\", \"novel\",\n",
    "                \"now\", \"nowhere\", \"o\", \"obviously\", \"of\", \"off\", \"often\", \"oh\",\n",
    "                \"ok\", \"okay\", \"old\", \"on\", \"once\", \"one\", \"ones\", \"only\",\n",
    "                \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"ought\", \"our\",\n",
    "                \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\",\n",
    "                \"own\", \"p\", \"particular\", \"particularly\", \"per\", \"perhaps\",\n",
    "                \"placed\", \"please\", \"plus\", \"possible\", \"presumably\",\n",
    "                \"probably\", \"provides\", \"q\", \"que\", \"quite\", \"qv\", \"r\",\n",
    "                \"rather\", \"rd\", \"re\", \"really\", \"reasonably\", \"regarding\",\n",
    "                \"regardless\", \"regards\", \"relatively\", \"respectively\", \"right\",\n",
    "                \"s\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"second\",\n",
    "                \"secondly\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\",\n",
    "                \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\",\n",
    "                \"serious\", \"seriously\", \"seven\", \"several\", \"shall\", \"she\",\n",
    "                \"should\", \"shouldn't\", \"since\", \"six\", \"so\", \"some\",\n",
    "                \"somebody\", \"somehow\", \"someone\", \"something\", \"sometime\",\n",
    "                \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\",\n",
    "                \"specified\", \"specify\", \"specifying\", \"still\", \"sub\", \"such\",\n",
    "                \"sup\", \"sure\", \"t\", \"t's\", \"take\", \"taken\", \"tell\", \"tends\",\n",
    "                \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that's\",\n",
    "                \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\",\n",
    "                \"then\", \"thence\", \"there\", \"there's\", \"thereafter\", \"thereby\",\n",
    "                \"therefore\", \"therein\", \"theres\", \"thereupon\", \"these\", \"they\",\n",
    "                \"they'd\", \"they'll\", \"they're\", \"they've\", \"think\", \"third\",\n",
    "                \"this\", \"thorough\", \"thoroughly\", \"those\", \"though\", \"three\",\n",
    "                \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                \"too\", \"took\", \"toward\", \"towards\", \"tried\", \"tries\", \"truly\",\n",
    "                \"try\", \"trying\", \"twice\", \"two\", \"u\", \"un\", \"under\",\n",
    "                \"unfortunately\", \"unless\", \"unlikely\", \"until\", \"unto\", \"up\",\n",
    "                \"upon\", \"us\", \"use\", \"used\", \"useful\", \"uses\", \"using\",\n",
    "                \"usually\", \"uucp\", \"v\", \"value\", \"various\", \"very\", \"via\",\n",
    "                \"viz\", \"vs\", \"w\", \"want\", \"wants\", \"was\", \"wasn't\", \"way\",\n",
    "                \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"welcome\", \"well\",\n",
    "                \"went\", \"were\", \"weren't\", \"what\", \"what's\", \"whatever\",\n",
    "                \"when\", \"whence\", \"whenever\", \"where\", \"where's\", \"whereafter\",\n",
    "                \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n",
    "                \"whether\", \"which\", \"while\", \"whither\", \"who\", \"who's\",\n",
    "                \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"willing\",\n",
    "                \"wish\", \"with\", \"within\", \"without\", \"won't\", \"wonder\",\n",
    "                \"would\", \"would\", \"wouldn't\", \"x\", \"y\", \"yes\", \"yet\", \"you\",\n",
    "                \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\",\n",
    "                \"yourself\", \"yourselves\", \"z\", \"zero\",\"quot\"]\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    Java_keyWords=[porter.stem(each.strip().lower()) for each in JavaKeywords]\n",
    "    natural_stop_words=[porter.stem(each.strip().lower()) for each in stop_words]\n",
    "    \n",
    "    processed_doc=[]\n",
    "    if mode==\"code\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in code_splitter(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in Java_keyWords or\n",
    "                                                            term in natural_stop_words or len(term)<2)]\n",
    "    elif mode==\"text\":\n",
    "        splitted_doc=[porter.stem(term.lower()) for term in split_natural_lang(doc)]\n",
    "        processed_doc=[term for term in splitted_doc if not(term in natural_stop_words or len(term)<2)]\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading source codes into pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classNames_methodNames(node):\n",
    "    result=''\n",
    "    if isinstance(node,javalang.tree.MethodDeclaration) or isinstance(node,javalang.tree.ClassDeclaration):\n",
    "        return node.name.lower()+' '\n",
    "    if not (isinstance(node,javalang.tree.PackageDeclaration) or\n",
    "        isinstance(node,javalang.tree.FormalParameter) or\n",
    "       isinstance(node,javalang.tree.Import)):\n",
    "        if node:\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=classNames_methodNames(childNode)\n",
    "    return result\n",
    "    \n",
    "def traverse_node(node,i=0):\n",
    "    i+=1\n",
    "    result=''\n",
    "    if not(isinstance(node,javalang.tree.PackageDeclaration)\n",
    "            or isinstance(node,javalang.tree.FormalParameter)            \n",
    "            or isinstance(node,javalang.tree.Import)\n",
    "            or isinstance(node,javalang.tree.CompilationUnit)):\n",
    "        if node:\n",
    "            if (isinstance(node,int) or isinstance(node,str) or isinstance(node,float)) and i==2:\n",
    "                result+=node+' '\n",
    "            if isinstance(node, javalang.ast.Node):\n",
    "                for childNode in node.children:\n",
    "                    result+=traverse_node(childNode,i)\n",
    "    return result\n",
    "\n",
    "def code_parser(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        return ''.join([traverse_node(node) for path, node in tree]) + ' ' + ''.join([classNames_methodNames(node)\n",
    "                                                                                      for path, node in tree])\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        return ''\n",
    "\n",
    "    \n",
    "@ray.remote\n",
    "def async_sourcecode_procesing(source_code):\n",
    "#     print(source_code)\n",
    "#     try:\n",
    "    sc_code = zlib.decompress(bytes.fromhex(source_code)).decode()\n",
    "#     except:\n",
    "#         print(source_code)\n",
    "# #     print(sc_code)\n",
    "    processed_code=general_preprocessor(code_parser(sc_code),'code')\n",
    "#     print(processed_code)\n",
    "    return zlib.compress(\" \".join(processed_code).encode(\"utf-8\")).hex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Loading bug reports pandas Dataframe</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def loadBugs2df(report):\n",
    "#     print(report)\n",
    "    return general_preprocessor(report,\"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_full_dataset(project_name, test_percentage=0.4):\n",
    "    df1 = pd.read_csv(\"{}.csv\".format(project_name), delimiter='\\t')\n",
    "    df2 = pd.read_csv(\"{}_features.csv\".format(project_name))\n",
    "    df3 = pd.read_csv(\"{}_features_file_content.csv\".format(project_name))\n",
    "    df4 = pd.merge(df2, df3, left_on='cid', right_on='cid', how='inner')\n",
    "    df5 = pd.merge(df1, df4, left_on='id', right_on='report_id', how='inner')\n",
    "    df5['report'] = df5['summary'] + df5['description']\n",
    "    df5['project_name'] = project_name.split(\"/\")[-1]\n",
    "    train_pos, test_pos = train_test_split(df5[df5['match'] == 1], test_size=test_percentage, random_state=13, shuffle=False)\n",
    "    train, test = df5[df5['bug_id'].isin(train_pos['bug_id'])], df5[df5['bug_id'].isin(test_pos['bug_id'])]\n",
    "    train = train.copy().reset_index(drop=True)\n",
    "    small_train = pd.DataFrame(columns=train.columns)\n",
    "    for item in train['bug_id'].unique():\n",
    "        temp = pd.concat((train[(train['bug_id'] == item) & (train['match'] == 1)],\n",
    "                          train[(train['bug_id'] == item) & (train['match'] == 0)].head(10)))\n",
    "        small_train = pd.concat((small_train, temp))\n",
    "    small_train.drop(columns=set(small_train.columns) - {'id', 'cid', 'report', 'file_content', 'match'}, inplace=True)\n",
    "    return small_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_dataset(dataset_list, full_size=1000):\n",
    "    part_size = 1.0 / len(dataset_list)\n",
    "    temp_df = pd.DataFrame(columns=dataset_list[0].columns)\n",
    "    for item in dataset_list:\n",
    "        temp_df = temp_df.append(\n",
    "            item.sample(frac=(full_size * part_size) / len(item), replace=False, random_state=13).reset_index(\n",
    "                drop=True)).reset_index(drop=True)\n",
    "    return temp_df.sample(frac=1, random_state=13).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h1>Main Preprocessing class</h1></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingUnit:\n",
    "\n",
    "    # all_projects_source_codes=pd.DataFrame([])\n",
    "    # all_projects_bugreports=pd.DataFrame([])\n",
    "    \n",
    "    \n",
    "\n",
    "    def __init__(self,project_name):\n",
    "#         project_df = ray_pd.DataFrame([])\n",
    "        self.dataPath=project_name\n",
    "        # self.dataFolder=os.path.join(os.getcwd(),'Data')\n",
    "        # if not os.path.exists(self.dataFolder):\n",
    "        #     os.makedirs(self.dataFolder)\n",
    "            \n",
    "    def execute(self):\n",
    "\n",
    "        self.loadEverything()\n",
    "\n",
    "    def loadEverything(self):\n",
    "#         vectorize=False\n",
    "#         if PreprocessingUnit.all_projects_bugreports.empty:\n",
    "#             bugReportFile=os.path.join(self.dataFolder,'allBugReports.csv')\n",
    "#             if not os.path.isfile(bugReportFile):\n",
    "#                 PreprocessingUnit.all_projects_bugreports=load_all_BRs(dataPath=self.dataPath)\n",
    "#                 vectorize=True\n",
    "#                 if vectorize:\n",
    "#                     PreprocessingUnit.all_projects_bugreports.to_csv(bugReportFile, index=False)\n",
    "#             else: \n",
    "#                 PreprocessingUnit.all_projects_bugreports=pd.read_pickle(bugReportFile)\n",
    "# #         if vectorize:\n",
    "# #             with open()\n",
    "#         print(\"*** All bug reports are are preprocessed and stored as: {} ***\".format('/'.join(bugReportFile.split('/')[-2:])))\n",
    "        start = time.time()\n",
    "        df1 = ray_pd.read_csv(\"{}.csv\".format(self.dataPath), delimiter='\\t')\n",
    "        df2 = ray_pd.read_csv(\"{}_features.csv\".format(self.dataPath))\n",
    "        df3 = ray_pd.read_csv(\"{}_features_file_content.csv\".format(self.dataPath))\n",
    "        df4 = ray_pd.merge(df2, df3, left_on='cid', right_on='cid', how='inner')\n",
    "        df5 = ray_pd.merge(df1, df4, left_on='id', right_on='report_id', how='inner')\n",
    "        df5['report'] = df5['summary'] + df5['description']\n",
    "        df5['project_name'] = self.dataPath.split(\"/\")[-1]\n",
    "        df5 = df5.head()\n",
    "#         modin_df = from_pandas(df5)\n",
    "        \n",
    "#         df5 = df5.head()\n",
    "#         ray_pd = ray_pd.from_pandas(df5)\n",
    "        reports = df5.report.values.tolist()\n",
    "        source_codes = df5.file_content.values.tolist()\n",
    "        processed_reports = []\n",
    "        processed_source_codes = []\n",
    "        for i, r in enumerate(reports):\n",
    "            if len(processed_reports) > 1000:\n",
    "                num_ready = i-1000\n",
    "                ray.wait(processed_reports, num_returns=num_ready)\n",
    "\n",
    "            processed_reports.append(loadBugs2df.remote(r))\n",
    "#             try:\n",
    "#                 processed_reports.append(loadBugs2df(r))\n",
    "#             except:\n",
    "#                 processed_reports.append(None)\n",
    "        for i, sc in enumerate(source_codes):\n",
    "            if len(processed_source_codes) > 1000:\n",
    "                    num_ready = i-1000\n",
    "                    ray.wait(processed_source_codes, num_returns=num_ready)\n",
    "            processed_source_codes.append(async_sourcecode_procesing.remote(sc))\n",
    "# #             try:\n",
    "# #                 processed_source_codes.append(async_sourcecode_procesing(sc))\n",
    "# #             except:\n",
    "# #                 processed_reports.append(None)\n",
    "        processed_reports_ = []\n",
    "        for pr in processed_reports:\n",
    "            try:\n",
    "                processed_reports_.append(ray.get(pr))\n",
    "            except:\n",
    "                processed_reports_.append(ray.get(None))\n",
    "        processed_reports = processed_reports_\n",
    "        processed_src_ = []\n",
    "        for pr_src in processed_source_codes:\n",
    "            try:\n",
    "                processed_src_.append(ray.get(pr_src))\n",
    "            except:\n",
    "                processed_src_.append(ray.get(None))\n",
    "        processed_reports = processed_reports_\n",
    "        processed_source_codes = processed_src_\n",
    "        df5[\"globug_processed_report\"] = processed_reports #df5.report.apply(lambda x: loadBugs2df.remote(x))\n",
    "        df5[\"globug_processed_file_content\"] = processed_source_codes #df5.file_content.apply(lambda x: async_sourcecode_procesing.remote(x))\n",
    "\n",
    "        processed_folder = os.path.join(\"/\".join(self.dataPath.split(\"/\")[:-2]), \"processed_bl_reports\")\n",
    "        pathlib.Path(processed_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        project = self.dataPath.split(\"/\")[-1]\n",
    "        project_processed_folder = os.path.join(processed_folder, project)\n",
    "        pathlib.Path(project_processed_folder).mkdir(parents=True, exist_ok=True)\n",
    "        project_processed_file = os.path.join(project_processed_folder, f\"{project}_processed.csv\")\n",
    "\n",
    "        df5.to_csv(project_processed_file, index=False)\n",
    "        end = time.time()\n",
    "        print(f\"Processing project: {project} completed\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def process_df(path):\n",
    "    pu = PreprocessingUnit(path)\n",
    "    pu.execute()\n",
    "    return pu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [\"Birt\", \"Tomcat\", \"JDT\", \"SWT\", \"Eclipse_Platform_UI\"]\n",
    "# projects = [\"AspectJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=2259)\u001b[0m UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "\u001b[2m\u001b[36m(pid=2259)\u001b[0m Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "\u001b[2m\u001b[36m(pid=2282)\u001b[0m UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "\u001b[2m\u001b[36m(pid=2282)\u001b[0m Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "\u001b[2m\u001b[36m(pid=2323)\u001b[0m UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "\u001b[2m\u001b[36m(pid=2323)\u001b[0m Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "\u001b[2m\u001b[36m(pid=2274)\u001b[0m UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "\u001b[2m\u001b[36m(pid=2274)\u001b[0m Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "\u001b[2m\u001b[36m(pid=2291)\u001b[0m UserWarning: `read_*` implementation has mismatches with pandas:\n",
      "\u001b[2m\u001b[36m(pid=2291)\u001b[0m Data types of partitions are different! Please refer to the troubleshooting section of the Modin documentation to fix this issue.\n",
      "2022-02-23 13:47:03,616\tWARNING worker.py:1189 -- This worker was asked to execute a function that it does not have registered. You may have to restart Ray.\n",
      "2022-02-23 13:48:58,947\tWARNING worker.py:1189 -- The node with node id: 65fdc0b7c4c9f6b0608ac24cab6d4d0ae09918a50c997464f01d6789 and ip: 172.16.138.255 has been marked dead because the detector has missed too many heartbeats from it. This can happen when a raylet crashes unexpectedly or has lagging heartbeats.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2022-02-23 13:49:11,150 C 2175 2175] node_manager.cc:165: This node has beem marked as dead.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     ray::SpdLogMessage::Flush()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     ray::rpc::ClientCallImpl<>::OnReplyReceived()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     boost::asio::io_context::run()\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     main\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m     __libc_start_main\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "processing = []\n",
    "for p in projects:\n",
    "    data_path = (f\"/home/varumuga/scratch/thesis/replication/bench_bl_dataset/Dataset/{p}\")\n",
    "    processing.append(process_df.remote(data_path))\n",
    "\n",
    "processing = ray.get(processing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"/home/varumuga/scratch/thesis/replication/bench_bl_dataset/processed_bl_reports/AspectJ_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(preprocessor.all_projects_bugreports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor.dataFolder\n",
    "# bugReportFile=os.path.join(preprocessor.dataFolder,'allBugReports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bugReportFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreprocessingUnit.all_projects_bugreports.to_csv(bugReportFile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.read_pickle(bugReportFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreprocessingUnit.all_projects_bugreports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
